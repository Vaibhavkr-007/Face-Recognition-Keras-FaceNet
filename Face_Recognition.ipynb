{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e0c74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 18:46:48.318428: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-31 18:46:48.328883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748697408.341234   34487 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748697408.345256   34487 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748697408.354537   34487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748697408.354555   34487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748697408.354556   34487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748697408.354557   34487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 18:46:48.357802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf0fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748697410.606215   34487 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1923 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "embedder = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f18ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = r\"/home/phoenix/ProjDD/Projects/P05_MobileNet_FaceRecognition/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1142b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    for filename in os.listdir(embedding_dir):\n",
    "        if filename.endswith(\".pkl\"):\n",
    "            label = filename.split(\".\")[0]\n",
    "            with open(os.path.join(embedding_dir, filename), \"rb\") as f:\n",
    "                embeddings[label] = pickle.load(f)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398c1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(frame, known_embeddings):\n",
    "    detections = embedder.extract(frame, threshold=0.95)\n",
    "    recognized_faces = []\n",
    "\n",
    "    for detection in detections:\n",
    "        embedding = detection['embedding']\n",
    "        box = detection['box']\n",
    "        similarities = {\n",
    "            label: cosine_similarity([embedding], [known_embedding])[0][0]\n",
    "            for label, known_embedding in known_embeddings.items()\n",
    "        }\n",
    "\n",
    "        best_match = max(similarities, key=similarities.get)\n",
    "        if similarities[best_match] > 0.8:\n",
    "            recognized_faces.append((best_match, box))\n",
    "        else:\n",
    "            recognized_faces.append((\"Unknown\", box))\n",
    "    \n",
    "    return recognized_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8721d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4fb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    known_embeddings = load_embeddings()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        recognized_faces = recognize_face(frame, known_embeddings)\n",
    "\n",
    "        for label, (x, y, width, height) in recognized_faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Real-Time Face Recognition', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ceb1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 18:46:58.002575: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/phoenix/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/cv2/qt/plugins\"\n",
      "2025-05-31 18:46:58.214580: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2025-05-31 18:46:58.504568: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2025-05-31 18:46:59.016044: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,24,24,3] vs. [1,1,1,28]\n",
      "2025-05-31 18:46:59.997946: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2025-05-31 18:47:02.081956: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2025-05-31 18:47:06.169658: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748697428.086433   36383 service.cc:152] XLA service 0x7dbb4c017180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748697428.086450   36383 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-05-31 18:47:08.173440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748697429.110497   36383 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748697432.874028   36383 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 18:47:23.117460: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m      5\u001b[39m         cap.release()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m recognized_faces = \u001b[43mrecognize_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label, (x, y, width, height) \u001b[38;5;129;01min\u001b[39;00m recognized_faces:\n\u001b[32m     12\u001b[39m     cv2.rectangle(frame, (x, y), (x + width, y + height), (\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m0\u001b[39m), \u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mrecognize_face\u001b[39m\u001b[34m(frame, known_embeddings)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecognize_face\u001b[39m(frame, known_embeddings):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     detections = \u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     recognized_faces = []\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m detections:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/keras_facenet/__init__.py:94\u001b[39m, in \u001b[36mFaceNet.extract\u001b[39m\u001b[34m(self, filepath_or_image, threshold)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath_or_image, threshold=\u001b[32m0.95\u001b[39m):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract faces and compute embeddings in one go. Requires\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    mtcnn to be installed.\u001b[39;00m\n\u001b[32m     86\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m \u001b[33;03m        with an \"embedding\" vector.\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     detections, crops = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detections:\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/keras_facenet/__init__.py:76\u001b[39m, in \u001b[36mFaceNet.crop\u001b[39m\u001b[34m(self, filepath_or_image, threshold)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m     image = filepath_or_image\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m detections = [detection \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m detection[\u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m] > threshold]\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detections:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/mtcnn/mtcnn.py:159\u001b[39m, in \u001b[36mMTCNN.detect_faces\u001b[39m\u001b[34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Process images through each stage (PNet, RNet, ONet)\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stages:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         bboxes_batch = \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_oshapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_oshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m tf.errors.InvalidArgumentError:  \u001b[38;5;66;03m# No faces found\u001b[39;00m\n\u001b[32m    162\u001b[39m     bboxes_batch = np.empty((\u001b[32m0\u001b[39m, \u001b[32m16\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/mtcnn/stages/stage_onet.py:82\u001b[39m, in \u001b[36mStageONet.__call__\u001b[39m\u001b[34m(self, images_normalized, bboxes_batch, threshold_onet, nms_onet, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03mRuns the ONet stage on the input images and bounding boxes, refining the proposals generated by the RNet stage\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03mand adding facial landmarks prediction.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m    np.ndarray: A numpy array of refined bounding boxes and landmarks after ONet processing, ready for the final stage.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# 1. Extract patches for each bounding box from the normalized images.\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# These patches are resized to the expected input size for ONet (48x48).\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m patches = \u001b[43mextract_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# 2. Pass the extracted patches through ONet to get bounding box offsets, facial landmarks, and confidence scores.\u001b[39;00m\n\u001b[32m     85\u001b[39m bboxes_offsets, face_landmarks, scores = \u001b[38;5;28mself\u001b[39m._model(patches)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/mtcnn/utils/images.py:397\u001b[39m, in \u001b[36mextract_patches\u001b[39m\u001b[34m(images_normalized, bboxes_batch, expected_size)\u001b[39m\n\u001b[32m    394\u001b[39m bboxes_batch_coords = bboxes_batch[:, selector] / np.asarray([[shape[selector[\u001b[32m1\u001b[39m]], shape[selector[\u001b[32m0\u001b[39m]], shape[selector[\u001b[32m1\u001b[39m]], shape[selector[\u001b[32m0\u001b[39m]]]])\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# Extract patches from the images using the bounding boxes, resizing them to `expected_size`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m result = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop_and_resize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Input image tensor\u001b[39;49;00m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbboxes_batch_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Bounding boxes in format [y1, x1, y2, x2], normalized to [0.0, 1.0]\u001b[39;49;00m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Indices of the images in the batch corresponding to the bounding boxes\u001b[39;49;00m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Size to resize the cropped patches (height, width)\u001b[39;49;00m\n\u001b[32m    402\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/ops/image_ops_impl.py:4936\u001b[39m, in \u001b[36mcrop_and_resize_v2\u001b[39m\u001b[34m(image, boxes, box_indices, crop_size, method, extrapolation_value, name)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mimage.crop_and_resize\u001b[39m\u001b[33m'\u001b[39m, v1=[])\n\u001b[32m   4832\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m   4833\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcrop_and_resize_v2\u001b[39m(image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4838\u001b[39m                        extrapolation_value=\u001b[32m.0\u001b[39m,\n\u001b[32m   4839\u001b[39m                        name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   4840\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Extracts crops from the input image tensor and resizes them.\u001b[39;00m\n\u001b[32m   4841\u001b[39m \n\u001b[32m   4842\u001b[39m \u001b[33;03m  Extracts crops from the input image tensor and resizes them using bilinear\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \n\u001b[32m   4935\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4936\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_image_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrop_and_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m                                       \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextrapolation_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/ops/gen_image_ops.py:498\u001b[39m, in \u001b[36mcrop_and_resize\u001b[39m\u001b[34m(image, boxes, box_ind, crop_size, method, extrapolation_value, name)\u001b[39m\n\u001b[32m    496\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcrop_and_resize_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m      \u001b[49m\u001b[43mextrapolation_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextrapolation_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._SymbolicException:\n\u001b[32m    502\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/ops/gen_image_ops.py:534\u001b[39m, in \u001b[36mcrop_and_resize_eager_fallback\u001b[39m\u001b[34m(image, boxes, box_ind, crop_size, method, extrapolation_value, name, ctx)\u001b[39m\n\u001b[32m    532\u001b[39m   extrapolation_value = \u001b[32m0\u001b[39m\n\u001b[32m    533\u001b[39m extrapolation_value = _execute.make_float(extrapolation_value, \u001b[33m\"\u001b[39m\u001b[33mextrapolation_value\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m _attr_T, (image,) = \u001b[43m_execute\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m boxes = _ops.convert_to_tensor(boxes, _dtypes.float32)\n\u001b[32m    536\u001b[39m box_ind = _ops.convert_to_tensor(box_ind, _dtypes.int32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:251\u001b[39m, in \u001b[36margs_to_matching_eager\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m   tensor = \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[32m    253\u001b[39m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[32m    254\u001b[39m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[32m    255\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m tensor.dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[39m, in \u001b[36m_constant_tensor_conversion_function\u001b[39m\u001b[34m(v, dtype, name, as_ref)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[32m     28\u001b[39m _ = as_ref\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[39m, in \u001b[36mconstant\u001b[39m\u001b[34m(value, dtype, shape, name)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mconstant\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconstant\u001b[39m(\n\u001b[32m    179\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, shape=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[33m\"\u001b[39m\u001b[33mConst\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m ) -> Union[ops.Operation, ops._EagerTensorBase]:\n\u001b[32m    181\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[32m    182\u001b[39m \n\u001b[32m    183\u001b[39m \u001b[33;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[39m, in \u001b[36m_constant_impl\u001b[39m\u001b[34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[33m\"\u001b[39m\u001b[33mtf.constant\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    288\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m const_tensor = ops._create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    292\u001b[39m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[39m, in \u001b[36m_constant_eager_impl\u001b[39m\u001b[34m(ctx, value, dtype, shape, verify_shape)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_constant_eager_impl\u001b[39m(\n\u001b[32m    298\u001b[39m     ctx, value, dtype, shape, verify_shape\n\u001b[32m    299\u001b[39m ) -> ops._EagerTensorBase:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m   t = \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ProjDD/MLJ/Machine_learning/ml/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d3c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
